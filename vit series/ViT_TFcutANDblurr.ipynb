{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hun9008/ML_TeamProject_24SS/blob/main/ViT_TFcutANDblurr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaQi2LTwPNco",
        "outputId": "7d359d7b-33e0-4701-a4c5-232d83d6d6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_T_aU6PdD1",
        "outputId": "ce83a72b-668e-4a0c-efb5-61ce43fb5882"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1G6SgIRKCYt5sUkkaCAiAmdP0_hZA-rFs/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('../../cut_resize_png')"
      ],
      "metadata": {
        "id": "_3zfoxPtPjlS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWj2jCh7DVVk",
        "outputId": "343cabad-5a4e-45c2-dae1-5e583c80b68e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mblurr_cut_png\u001b[0m/   data_2000.zip     \u001b[01;34mimages_cut_jpg\u001b[0m/      \u001b[01;34mimages_gray_png\u001b[0m/     images_jpg.zip\n",
            "\u001b[01;34mcut_resize_jpg\u001b[0m/  \u001b[01;34mgray_cut_jpg\u001b[0m/     \u001b[01;34mimages_gray_jpg\u001b[0m/     images_gray_png.zip  \u001b[01;34mimages_png\u001b[0m/\n",
            "\u001b[01;34mcut_resize_png\u001b[0m/  \u001b[01;34mimages_cut2_png\u001b[0m/  images_gray_jpg.zip  \u001b[01;34mimages_jpg\u001b[0m/          images_png.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ViT30nkxZ9d",
        "outputId": "2e479a52-62dd-4364-9513-2966503cddd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyhSJeDjyEFu",
        "outputId": "f909946d-6d4a-43cd-81c0-6b7f5914018e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/611.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vit-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUC4cCHpyNfT",
        "outputId": "a2d99054-e0fe-4fab-9eb4-121770126ec3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.4)\n",
            "Collecting validators (from vit-keras)\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.25.2)\n",
            "Installing collected packages: validators, vit-keras\n",
            "Successfully installed validators-0.28.1 vit-keras-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow_addons as tfa\n",
        "from vit_keras import vit\n",
        "from vit_keras import visualize\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE16NJ5TPlN_",
        "outputId": "6b31914f-9c32-4837-ee4c-3f4ccaa60980"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_jpg_path = \"/content/drive/My Drive/ML_TeamProject/images_jpg\"\n",
        "image_png_path = \"/content/drive/My Drive/ML_TeamProject/images_png\"\n",
        "image_gray_jpg_path = \"/content/drive/My Drive/ML_TeamProject/images_gray_jpg\"\n",
        "image_gray_png_path = \"/content/drive/My Drive/ML_TeamProject/images_gray_png\"\n",
        "image_cut2_png_path = '/content/drive/My Drive/data/images_cut2_png'\n",
        "cut_resize_png_path = '/content/drive/My Drive/data/cut_resize_png'\n",
        "cut_resize_jpg_path = '/content/drive/My Drive/data/cut_resize_jpg'\n",
        "blurr_cut_png_path = '/content/drive/My Drive/data/blurr_cut_png'"
      ],
      "metadata": {
        "id": "34-qwn4kPr92"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/data/images_cut2_png\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJHViDgyXvON",
        "outputId": "f086961a-b599-498c-acb9-90bbf90df2a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "incipient  mature  no  overripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q images_gray_jpg.zip"
      ],
      "metadata": {
        "id": "mviidsa8f6d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-7FLrXEgifM",
        "outputId": "248df163-db2f-43db-fc44-8843bbc5c470"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "incipient  mature  no  overripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = blurr_cut_png_path"
      ],
      "metadata": {
        "id": "DVt1LWReYSTP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.rename('overripe_train_labeled_2000','overripe')\n",
        "# os.rename('no_train_labeled_2000','no')\n",
        "# os.rename('mature_train_labeled_2000','mature')\n",
        "# os.rename('incipient_train_labeled_2000','incipient')"
      ],
      "metadata": {
        "id": "0ieRIPFVa0O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrQQu92guqG",
        "outputId": "f5eb76d4-2fd6-400a-e1ed-75b986c6b233"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blurr_cut_png\tdata_2000.zip\t images_cut_jpg       images_gray_png\t   images_jpg.zip\n",
            "cut_resize_jpg\tgray_cut_jpg\t images_gray_jpg      images_gray_png.zip  images_png\n",
            "cut_resize_png\timages_cut2_png  images_gray_jpg.zip  images_jpg\t   images_png.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickle load to variable"
      ],
      "metadata": {
        "id": "dNicedTHaWLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(image_path, 'overripe'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     overripe = pickle.load(f)\n",
        "     print('overripe : ', len(overripe))\n",
        "os.chdir(os.path.join(image_path, 'no'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     no = pickle.load(f)\n",
        "     print('no : ', len(no))\n",
        "\n",
        "os.chdir(os.path.join(image_path, 'mature'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     mature = pickle.load(f)\n",
        "     print('mature : ', len(mature))\n",
        "\n",
        "os.chdir(os.path.join(image_path, 'incipient'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     incipient = pickle.load(f)\n",
        "     print('incipient : ', len(incipient))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxkl8HkPXz1y",
        "outputId": "00527eec-b481-4a1a-bc89-8964c5985aef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overripe :  2000\n",
            "no :  2000\n",
            "mature :  2000\n",
            "incipient :  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary to numpy array"
      ],
      "metadata": {
        "id": "LIY8mkCtabv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overripe의 모든 사진에 대해 반복\n",
        "overripe_data_list = []\n",
        "\n",
        "for filename, data in overripe.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    overripe_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "overripe_data = np.array(overripe_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"overripe_data shape:\", overripe_data.shape)\n",
        "no_data_list = []\n",
        "\n",
        "for filename, data in no.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    no_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "no_data = np.array(no_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"no_data shape:\", no_data.shape)\n",
        "\n",
        "mature_data_list = []\n",
        "\n",
        "for filename, data in mature.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    mature_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "mature_data = np.array(mature_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"mature_data shape:\", mature_data.shape)\n",
        "\n",
        "incipient_data_list = []\n",
        "\n",
        "for filename, data in incipient.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    incipient_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "incipient_data = np.array(incipient_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"incipient_data shape:\", incipient_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxMGqb3NYVh1",
        "outputId": "5157af25-9c72-475d-8f22-0861656342ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overripe_data shape: (2000, 224, 224, 3)\n",
            "no_data shape: (2000, 224, 224, 3)\n",
            "mature_data shape: (2000, 224, 224, 3)\n",
            "incipient_data shape: (2000, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Val Test split (0.75:0.15:0.15)"
      ],
      "metadata": {
        "id": "6QXUwoi-akaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 train/validation/test로 나누기\n",
        "X = np.concatenate((overripe_data, no_data, mature_data, incipient_data), axis=0)\n",
        "y = np.concatenate((np.zeros(overripe_data.shape[0]), np.ones(no_data.shape[0]),\n",
        "                    2*np.ones(mature_data.shape[0]), 3*np.ones(incipient_data.shape[0])))\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsXIqEfraDw-",
        "outputId": "d7e8d3f1-e182-48ed-cb51-5574fc12b1cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5600, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y one-hot"
      ],
      "metadata": {
        "id": "fC5k-MspbVG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_val_one_hot = to_categorical(y_val)\n",
        "y_test_one_hot = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "MPyh4QsebQm_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory reduce\n",
        "del overripe, no, mature, incipient\n",
        "del overripe_data, no_data, mature_data, incipient_data\n",
        "del overripe_data_list, no_data_list, mature_data_list, incipient_data_list\n",
        "del X, y\n",
        "del y_train, y_temp, y_val, y_test"
      ],
      "metadata": {
        "id": "1LZA8OcqbdZm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Test"
      ],
      "metadata": {
        "id": "lhdB7e9DckDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = vit.vit_b32(\n",
        "        image_size = 224,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMC5meL0UMjy",
        "outputId": "9abcd9b1-000f-49de-d602-821e7c6cc285"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_32_imagenet21k+imagenet2012.npz\n",
            "353253686/353253686 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYFoqkEeXQ1o",
        "outputId": "e53a16c4-8e10-4646-95e5-19c04756137e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5600, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1salmoP_XgH4",
        "outputId": "c1e1241e-1925-494b-9048-a4afa4027f18"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_vgg16_model():\n",
        "\n",
        "  # Freeze the base model layers\n",
        "  # for layer in base_model.layers:\n",
        "  #     layer.trainable = False\n",
        "\n",
        "# Add custom top layers for classification\n",
        "model = tf.keras.Sequential([\n",
        "        vit_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(4, 'softmax')\n",
        "    ],\n",
        "    name = 'vision_transformer')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "  # Compile model\n",
        "  # optimizer = Adam(lr=5.520238899015578e-05)\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
        "\n",
        "model.compile(optimizer = optimizer,\n",
        "                loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),\n",
        "                metrics = ['accuracy'])\n",
        "batch_size = 16\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                                factor = 0.2,\n",
        "                                                patience = 2,\n",
        "                                                verbose = 1,\n",
        "                                                min_delta = 1e-4,\n",
        "                                                min_lr = 1e-6,\n",
        "                                                mode = 'max')\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                min_delta = 1e-4,\n",
        "                                                patience = 5,\n",
        "                                                mode = 'max',\n",
        "                                                restore_best_weights = True,\n",
        "                                                verbose = 1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
        "                                                  monitor = 'val_accuracy',\n",
        "                                                  verbose = 1,\n",
        "                                                  save_best_only = True,\n",
        "                                                  save_weights_only = True,\n",
        "                                                  mode = 'max')\n",
        "\n",
        "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
        "\n",
        "model.fit(x = X_train,\n",
        "          y = y_train_one_hot,\n",
        "          # steps_per_epoch = batch_size,\n",
        "          validation_data = (X_val, y_val_one_hot),\n",
        "          # validation_steps = batch_size,\n",
        "          epochs = 50,\n",
        "          callbacks = callbacks)\n"
      ],
      "metadata": {
        "id": "G6Cei5K9cgft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d22986-457f-45f6-b3c9-b35ee5d30d4b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vision_transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vit-b32 (Functional)        (None, 768)               87455232  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 768)               0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 768)               3072      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11)                8459      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 11)                44        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87466855 (333.66 MB)\n",
            "Trainable params: 87465297 (333.65 MB)\n",
            "Non-trainable params: 1558 (6.09 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.1925 - accuracy: 0.6023\n",
            "Epoch 1: val_accuracy improved from -inf to 0.70000, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 50s 147ms/step - loss: 1.1925 - accuracy: 0.6023 - val_loss: 1.0690 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.9114 - accuracy: 0.7750\n",
            "Epoch 2: val_accuracy improved from 0.70000 to 0.74833, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 21s 121ms/step - loss: 0.9114 - accuracy: 0.7750 - val_loss: 0.9565 - val_accuracy: 0.7483 - lr: 1.0000e-04\n",
            "Epoch 3/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.8493 - accuracy: 0.8254\n",
            "Epoch 3: val_accuracy improved from 0.74833 to 0.82250, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 24s 140ms/step - loss: 0.8493 - accuracy: 0.8254 - val_loss: 0.8564 - val_accuracy: 0.8225 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.8063 - accuracy: 0.8593\n",
            "Epoch 4: val_accuracy did not improve from 0.82250\n",
            "175/175 [==============================] - 20s 115ms/step - loss: 0.8063 - accuracy: 0.8593 - val_loss: 0.9084 - val_accuracy: 0.8167 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.7743 - accuracy: 0.8825\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\n",
            "Epoch 5: val_accuracy did not improve from 0.82250\n",
            "175/175 [==============================] - 20s 114ms/step - loss: 0.7743 - accuracy: 0.8825 - val_loss: 0.8699 - val_accuracy: 0.8092 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.9200\n",
            "Epoch 6: val_accuracy improved from 0.82250 to 0.83083, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 24s 137ms/step - loss: 0.7259 - accuracy: 0.9200 - val_loss: 0.8504 - val_accuracy: 0.8308 - lr: 2.0000e-05\n",
            "Epoch 7/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.7044 - accuracy: 0.9375\n",
            "Epoch 7: val_accuracy did not improve from 0.83083\n",
            "175/175 [==============================] - 20s 114ms/step - loss: 0.7044 - accuracy: 0.9375 - val_loss: 0.8679 - val_accuracy: 0.8283 - lr: 2.0000e-05\n",
            "Epoch 8/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.9502\n",
            "Epoch 8: val_accuracy improved from 0.83083 to 0.83250, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 24s 138ms/step - loss: 0.6893 - accuracy: 0.9502 - val_loss: 0.8721 - val_accuracy: 0.8325 - lr: 2.0000e-05\n",
            "Epoch 9/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6766 - accuracy: 0.9623\n",
            "Epoch 9: val_accuracy did not improve from 0.83250\n",
            "175/175 [==============================] - 20s 114ms/step - loss: 0.6766 - accuracy: 0.9623 - val_loss: 0.8898 - val_accuracy: 0.8133 - lr: 2.0000e-05\n",
            "Epoch 10/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6658 - accuracy: 0.9695\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "\n",
            "Epoch 10: val_accuracy did not improve from 0.83250\n",
            "175/175 [==============================] - 20s 114ms/step - loss: 0.6658 - accuracy: 0.9695 - val_loss: 0.9016 - val_accuracy: 0.8067 - lr: 2.0000e-05\n",
            "Epoch 11/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6532 - accuracy: 0.9771\n",
            "Epoch 11: val_accuracy did not improve from 0.83250\n",
            "175/175 [==============================] - 20s 114ms/step - loss: 0.6532 - accuracy: 0.9771 - val_loss: 0.8842 - val_accuracy: 0.8275 - lr: 4.0000e-06\n",
            "Epoch 12/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6472 - accuracy: 0.9809\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\n",
            "Epoch 12: val_accuracy did not improve from 0.83250\n",
            "175/175 [==============================] - 20s 114ms/step - loss: 0.6472 - accuracy: 0.9809 - val_loss: 0.8818 - val_accuracy: 0.8292 - lr: 4.0000e-06\n",
            "Epoch 13/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6459 - accuracy: 0.9836Restoring model weights from the end of the best epoch: 8.\n",
            "\n",
            "Epoch 13: val_accuracy did not improve from 0.83250\n",
            "175/175 [==============================] - 20s 116ms/step - loss: 0.6459 - accuracy: 0.9836 - val_loss: 0.8794 - val_accuracy: 0.8267 - lr: 1.0000e-06\n",
            "Epoch 13: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b67d5edf7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZbr4a9fcdg7",
        "outputId": "2c62814e-bee2-4902-e397-f398265b1ba7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 2s 47ms/step - loss: 0.8511 - accuracy: 0.8475\n",
            "Test accuracy: 0.8475000262260437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YNSHoEOT00EM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
