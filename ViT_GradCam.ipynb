{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9gMPXAYI5dxsZHTVnbZbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hun9008/ML_TeamProject_24SS/blob/main/ViT_%08GradCam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaQi2LTwPNco",
        "outputId": "a0a1fe4b-66ba-43af-c6b6-17b32561a827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/ML_TeamProject/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_T_aU6PdD1",
        "outputId": "598281a0-07a0-4a19-fe78-c370621a263b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML_TeamProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3zfoxPtPjlS",
        "outputId": "91747549-7f0f-4029-ae61-852429b2ab2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/             \u001b[01;34mimages_png\u001b[0m/                                        VGG.ipynb\n",
            "\u001b[01;34mimages_gray_jpg\u001b[0m/  \u001b[01;34mpreprocessed_images_40\u001b[0m/                            \u001b[01;34m무_train_labeled_2000\u001b[0m/\n",
            "\u001b[01;34mimages_gray_png\u001b[0m/  PreTest.ipynb\n",
            "\u001b[01;34mimages_jpg\u001b[0m/       vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnoIBcVSIQP-",
        "outputId": "39f356cb-a246-4e9b-d61f-7d60e339b5ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlzpdVIEIZjN",
        "outputId": "ff4a1f10-9baa-426b-8e5d-269890d3740e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf_eWCSqId78",
        "outputId": "6e8b51b4-9153-4031-8adc-83fca2fca71a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit_keras\n",
            "  Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.4)\n",
            "Collecting validators (from vit_keras)\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.25.2)\n",
            "Installing collected packages: validators, vit_keras\n",
            "Successfully installed validators-0.28.1 vit_keras-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pickle\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow_addons as tfa\n",
        "from vit_keras import vit\n",
        "from vit_keras import visualize\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE16NJ5TPlN_",
        "outputId": "ca9668eb-7de7-4f83-8916-91c9745c4468"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_jpg_path = \"/content/drive/My Drive/ML_TeamProject/images_jpg\"\n",
        "image_png_path = \"/content/drive/My Drive/ML_TeamProject/images_png\"\n",
        "image_gray_jpg_path = \"/content/drive/My Drive/ML_TeamProject/images_gray_jpg\"\n",
        "image_gray_png_path = \"/content/drive/My Drive/ML_TeamProject/images_gray_png\""
      ],
      "metadata": {
        "id": "34-qwn4kPr92"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/My Drive/ML_TeamProject/images_png\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJHViDgyXvON",
        "outputId": "7cd9d670-566a-4df9-f3dc-d178cb53c96f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images_png.zip\tincipient  mature  no  overripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q images_gray_jpg.zip"
      ],
      "metadata": {
        "id": "mviidsa8f6d_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-7FLrXEgifM",
        "outputId": "6a35d1ce-5061-4da7-ea2d-59dd1b13eeba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images_png.zip\tincipient  mature  no  overripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = image_png_path"
      ],
      "metadata": {
        "id": "DVt1LWReYSTP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os.rename('overripe_train_labeled_2000','overripe')\n",
        "# os.rename('no_train_labeled_2000','no')\n",
        "# os.rename('mature_train_labeled_2000','mature')\n",
        "# os.rename('incipient_train_labeled_2000','incipient')"
      ],
      "metadata": {
        "id": "0ieRIPFVa0O2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnrQQu92guqG",
        "outputId": "c9c96e1b-c388-4645-9594-8192ff34d136"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images_png.zip\tincipient  mature  no  overripe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickle load to variable"
      ],
      "metadata": {
        "id": "dNicedTHaWLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(image_path, 'overripe'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     overripe = pickle.load(f)\n",
        "     print('overripe : ', len(overripe))\n",
        "os.chdir(os.path.join(image_path, 'no'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     no = pickle.load(f)\n",
        "     print('no : ', len(no))\n",
        "\n",
        "os.chdir(os.path.join(image_path, 'mature'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     mature = pickle.load(f)\n",
        "     print('mature : ', len(mature))\n",
        "\n",
        "os.chdir(os.path.join(image_path, 'incipient'))\n",
        "with open('zero_centering.pkl', 'rb') as f:\n",
        "     incipient = pickle.load(f)\n",
        "     print('incipient : ', len(incipient))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxkl8HkPXz1y",
        "outputId": "d58afe52-cdf0-4a1a-e67d-ae89e2367ad9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overripe :  2000\n",
            "no :  2000\n",
            "mature :  2000\n",
            "incipient :  2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary to numpy array"
      ],
      "metadata": {
        "id": "LIY8mkCtabv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overripe의 모든 사진에 대해 반복\n",
        "overripe_data_list = []\n",
        "\n",
        "for filename, data in overripe.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    overripe_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "overripe_data = np.array(overripe_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"overripe_data shape:\", overripe_data.shape)\n",
        "no_data_list = []\n",
        "\n",
        "for filename, data in no.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    no_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "no_data = np.array(no_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"no_data shape:\", no_data.shape)\n",
        "\n",
        "mature_data_list = []\n",
        "\n",
        "for filename, data in mature.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    mature_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "mature_data = np.array(mature_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"mature_data shape:\", mature_data.shape)\n",
        "\n",
        "incipient_data_list = []\n",
        "\n",
        "for filename, data in incipient.items():\n",
        "    zero_centering_value = data.get('zero_centering')\n",
        "    incipient_data_list.append(np.array(zero_centering_value))\n",
        "\n",
        "# 리스트를 NumPy 배열로 변환\n",
        "incipient_data = np.array(incipient_data_list)\n",
        "\n",
        "# 확인을 위해 배열의 크기 출력\n",
        "print(\"incipient_data shape:\", incipient_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxMGqb3NYVh1",
        "outputId": "60cbe88c-39a5-4895-a76c-1d5f50044258"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "overripe_data shape: (2000, 224, 224, 3)\n",
            "no_data shape: (2000, 224, 224, 3)\n",
            "mature_data shape: (2000, 224, 224, 3)\n",
            "incipient_data shape: (2000, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Val Test split (0.75:0.15:0.15)"
      ],
      "metadata": {
        "id": "6QXUwoi-akaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 train/validation/test로 나누기\n",
        "X = np.concatenate((overripe_data, no_data, mature_data, incipient_data), axis=0)\n",
        "y = np.concatenate((np.zeros(overripe_data.shape[0]), np.ones(no_data.shape[0]),\n",
        "                    2*np.ones(mature_data.shape[0]), 3*np.ones(incipient_data.shape[0])))\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsXIqEfraDw-",
        "outputId": "b34e8eb9-82ba-45cc-9e05-099b2abc63d9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5600, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y one-hot"
      ],
      "metadata": {
        "id": "fC5k-MspbVG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_one_hot = to_categorical(y_train)\n",
        "y_val_one_hot = to_categorical(y_val)\n",
        "y_test_one_hot = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "MPyh4QsebQm_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memory reduce\n",
        "del overripe, no, mature, incipient\n",
        "del overripe_data, no_data, mature_data, incipient_data\n",
        "del overripe_data_list, no_data_list, mature_data_list, incipient_data_list\n",
        "del X, y\n",
        "del y_train, y_temp, y_val, y_test"
      ],
      "metadata": {
        "id": "1LZA8OcqbdZm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Test"
      ],
      "metadata": {
        "id": "lhdB7e9DckDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = vit.vit_b32(\n",
        "        image_size = 224,\n",
        "        activation = 'softmax',\n",
        "        pretrained = True,\n",
        "        include_top = False,\n",
        "        pretrained_top = False,\n",
        "        classes = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMC5meL0UMjy",
        "outputId": "eaeb4c9e-7257-4ae8-b53e-56abf0acd6d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/faustomorales/vit-keras/releases/download/dl/ViT-B_32_imagenet21k+imagenet2012.npz\n",
            "353253686/353253686 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 12, 12 to 7, 7\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYFoqkEeXQ1o",
        "outputId": "e4af726e-42e4-49ec-be76-70682f128849"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5600, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n",
            "(1200, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1salmoP_XgH4",
        "outputId": "1eedc1c7-bc2b-4a3c-9fa1-138c56498561"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_vgg16_model():\n",
        "\n",
        "  # Freeze the base model layers\n",
        "  # for layer in base_model.layers:\n",
        "  #     layer.trainable = False\n",
        "\n",
        "# Add custom top layers for classification\n",
        "model = tf.keras.Sequential([\n",
        "        vit_model,\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Dense(4, 'softmax')\n",
        "    ],\n",
        "    name = 'vision_transformer')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "  # Compile model\n",
        "  # optimizer = Adam(lr=5.520238899015578e-05)\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n",
        "\n",
        "model.compile(optimizer = optimizer,\n",
        "                loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2),\n",
        "                metrics = ['accuracy'])\n",
        "batch_size = 16\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',\n",
        "                                                factor = 0.2,\n",
        "                                                patience = 2,\n",
        "                                                verbose = 1,\n",
        "                                                min_delta = 1e-4,\n",
        "                                                min_lr = 1e-6,\n",
        "                                                mode = 'max')\n",
        "\n",
        "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                min_delta = 1e-4,\n",
        "                                                patience = 5,\n",
        "                                                mode = 'max',\n",
        "                                                restore_best_weights = True,\n",
        "                                                verbose = 1)\n",
        "\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
        "                                                  monitor = 'val_accuracy',\n",
        "                                                  verbose = 1,\n",
        "                                                  save_best_only = True,\n",
        "                                                  save_weights_only = True,\n",
        "                                                  mode = 'max')\n",
        "\n",
        "callbacks = [earlystopping, reduce_lr, checkpointer]\n",
        "\n",
        "model.fit(x = X_train,\n",
        "          y = y_train_one_hot,\n",
        "          # steps_per_epoch = batch_size,\n",
        "          validation_data = (X_val, y_val_one_hot),\n",
        "          # validation_steps = batch_size,\n",
        "          epochs = 50,\n",
        "          callbacks = callbacks)\n"
      ],
      "metadata": {
        "id": "G6Cei5K9cgft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e7c6c9-3a3d-401d-c89b-18d043f6f2d9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vision_transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vit-b32 (Functional)        (None, 768)               87455232  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 768)               0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 768)               3072      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 11)                8459      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 11)                44        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 48        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 87466855 (333.66 MB)\n",
            "Trainable params: 87465297 (333.65 MB)\n",
            "Non-trainable params: 1558 (6.09 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.2416 - accuracy: 0.5255\n",
            "Epoch 1: val_accuracy improved from -inf to 0.65583, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 100s 447ms/step - loss: 1.2416 - accuracy: 0.5255 - val_loss: 1.0728 - val_accuracy: 0.6558 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 1.0142 - accuracy: 0.6913\n",
            "Epoch 2: val_accuracy improved from 0.65583 to 0.71583, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 74s 421ms/step - loss: 1.0142 - accuracy: 0.6913 - val_loss: 1.0004 - val_accuracy: 0.7158 - lr: 1.0000e-04\n",
            "Epoch 3/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.9437 - accuracy: 0.7504\n",
            "Epoch 3: val_accuracy improved from 0.71583 to 0.73750, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 74s 423ms/step - loss: 0.9437 - accuracy: 0.7504 - val_loss: 0.9725 - val_accuracy: 0.7375 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.8952 - accuracy: 0.7904\n",
            "Epoch 4: val_accuracy improved from 0.73750 to 0.74750, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 73s 420ms/step - loss: 0.8952 - accuracy: 0.7904 - val_loss: 0.9463 - val_accuracy: 0.7475 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.8468 - accuracy: 0.8227\n",
            "Epoch 5: val_accuracy did not improve from 0.74750\n",
            "175/175 [==============================] - 69s 392ms/step - loss: 0.8468 - accuracy: 0.8227 - val_loss: 0.9750 - val_accuracy: 0.7250 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.7972 - accuracy: 0.8639\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "\n",
            "Epoch 6: val_accuracy did not improve from 0.74750\n",
            "175/175 [==============================] - 68s 391ms/step - loss: 0.7972 - accuracy: 0.8639 - val_loss: 1.0292 - val_accuracy: 0.7092 - lr: 1.0000e-04\n",
            "Epoch 7/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.7379 - accuracy: 0.9120\n",
            "Epoch 7: val_accuracy improved from 0.74750 to 0.77750, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 75s 427ms/step - loss: 0.7379 - accuracy: 0.9120 - val_loss: 0.9121 - val_accuracy: 0.7775 - lr: 2.0000e-05\n",
            "Epoch 8/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.7058 - accuracy: 0.9407\n",
            "Epoch 8: val_accuracy improved from 0.77750 to 0.77833, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 74s 422ms/step - loss: 0.7058 - accuracy: 0.9407 - val_loss: 0.9167 - val_accuracy: 0.7783 - lr: 2.0000e-05\n",
            "Epoch 9/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.9441\n",
            "Epoch 9: val_accuracy improved from 0.77833 to 0.78167, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 76s 433ms/step - loss: 0.6949 - accuracy: 0.9441 - val_loss: 0.9189 - val_accuracy: 0.7817 - lr: 2.0000e-05\n",
            "Epoch 10/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6836 - accuracy: 0.9564\n",
            "Epoch 10: val_accuracy improved from 0.78167 to 0.78500, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 74s 421ms/step - loss: 0.6836 - accuracy: 0.9564 - val_loss: 0.9207 - val_accuracy: 0.7850 - lr: 2.0000e-05\n",
            "Epoch 11/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6671 - accuracy: 0.9688\n",
            "Epoch 11: val_accuracy did not improve from 0.78500\n",
            "175/175 [==============================] - 69s 391ms/step - loss: 0.6671 - accuracy: 0.9688 - val_loss: 0.9372 - val_accuracy: 0.7758 - lr: 2.0000e-05\n",
            "Epoch 12/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6601 - accuracy: 0.9709\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
            "\n",
            "Epoch 12: val_accuracy did not improve from 0.78500\n",
            "175/175 [==============================] - 68s 391ms/step - loss: 0.6601 - accuracy: 0.9709 - val_loss: 0.9409 - val_accuracy: 0.7817 - lr: 2.0000e-05\n",
            "Epoch 13/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6449 - accuracy: 0.9836\n",
            "Epoch 13: val_accuracy improved from 0.78500 to 0.79000, saving model to ./model.hdf5\n",
            "175/175 [==============================] - 75s 430ms/step - loss: 0.6449 - accuracy: 0.9836 - val_loss: 0.9270 - val_accuracy: 0.7900 - lr: 4.0000e-06\n",
            "Epoch 14/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.9843\n",
            "Epoch 14: val_accuracy did not improve from 0.79000\n",
            "175/175 [==============================] - 69s 392ms/step - loss: 0.6396 - accuracy: 0.9843 - val_loss: 0.9339 - val_accuracy: 0.7842 - lr: 4.0000e-06\n",
            "Epoch 15/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.9855\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
            "\n",
            "Epoch 15: val_accuracy did not improve from 0.79000\n",
            "175/175 [==============================] - 68s 391ms/step - loss: 0.6401 - accuracy: 0.9855 - val_loss: 0.9271 - val_accuracy: 0.7867 - lr: 4.0000e-06\n",
            "Epoch 16/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.9904\n",
            "Epoch 16: val_accuracy did not improve from 0.79000\n",
            "175/175 [==============================] - 68s 390ms/step - loss: 0.6328 - accuracy: 0.9904 - val_loss: 0.9308 - val_accuracy: 0.7875 - lr: 1.0000e-06\n",
            "Epoch 17/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.9889\n",
            "Epoch 17: val_accuracy did not improve from 0.79000\n",
            "175/175 [==============================] - 68s 391ms/step - loss: 0.6361 - accuracy: 0.9889 - val_loss: 0.9315 - val_accuracy: 0.7883 - lr: 1.0000e-06\n",
            "Epoch 18/50\n",
            "175/175 [==============================] - ETA: 0s - loss: 0.6346 - accuracy: 0.9880Restoring model weights from the end of the best epoch: 13.\n",
            "\n",
            "Epoch 18: val_accuracy did not improve from 0.79000\n",
            "175/175 [==============================] - 69s 393ms/step - loss: 0.6346 - accuracy: 0.9880 - val_loss: 0.9322 - val_accuracy: 0.7892 - lr: 1.0000e-06\n",
            "Epoch 18: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f2c0415cf10>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZbr4a9fcdg7",
        "outputId": "f7f9022c-6cfc-4f87-ab61-02e4a8737b84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 5s 118ms/step - loss: 0.9234 - accuracy: 0.7867\n",
            "Test accuracy: 0.7866666913032532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jacobgil/pytorch-grad-cam.git\n",
        "!cd pytorch-grad-cam\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkMoV9CVuiK0",
        "outputId": "be99cb9e-d35c-4b17-ebfe-66a9395908ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pytorch-grad-cam' already exists and is not an empty directory.\n",
            "Obtaining file:///content/drive/MyDrive/ML_TeamProject/images_png/incipient\n",
            "\u001b[31mERROR: file:///content/drive/MyDrive/ML_TeamProject/images_png/incipient does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd pytorch-grad-cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeIg9symwjlQ",
        "outputId": "2bd802e3-ec2c-438f-da56-c4d77485fb5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML_TeamProject/images_png/incipient/pytorch-grad-cam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcp68L01wmDO",
        "outputId": "b022ede8-b016-4379-aa4e-18ef8c2d6397"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/ML_TeamProject/images_png/incipient/pytorch-grad-cam\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (9.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (0.17.1+cu121)\n",
            "Collecting ttach (from grad-cam==1.5.0)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (4.66.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam==1.5.0) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam==1.5.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->grad-cam==1.5.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam==1.5.0) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam==1.5.0) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam==1.5.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam==1.5.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam==1.5.0) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam==1.5.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam==1.5.0) (1.3.0)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building editable for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.0-0.editable-py3-none-any.whl size=8969 sha256=e525e607a63192fc1a97a79500278daed9d983f1613220ad4b0a62a29a963094\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1qkn3krn/wheels/02/e6/6c/4e1b35d05537d49c668a3138e64a1bdbf3845032d2a981ed01\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, grad-cam\n",
            "Successfully installed grad-cam-1.5.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "ZjprvGalgjmD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vit_grad_cam(image_path, class_idx):\n",
        "    # Load ViT model\n",
        "    config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k', output_attentions=True)\n",
        "    model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k', config=config)\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess image\n",
        "    transform = Compose([Resize((224, 224)), ToTensor(), Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_pil = Image.fromarray(image)\n",
        "    image_tensor = transform(image_pil)\n",
        "    image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Grad-CAM calculation\n",
        "    target_layers = [model.encoder.layer[-1]]  # Use the last encoder layer\n",
        "    cam = GradCAM(model=model, target_layers=target_layers)\n",
        "    grayscale_cam, model_output = cam(input_tensor=image_tensor)\n",
        "\n",
        "    # Get the class prediction\n",
        "    predicted_class = model_output.logits.argmax(dim=1)\n",
        "\n",
        "    # Visualize Grad-CAM for the predicted class\n",
        "    grayscale_cam = grayscale_cam[0, predicted_class[0].item()]\n",
        "    cam_image = show_cam_on_image(image_tensor[0].permute(1, 2, 0).numpy(), grayscale_cam, use_rgb=True)\n",
        "\n",
        "    return cam_image"
      ],
      "metadata": {
        "id": "J3-Ip1YJgj9g"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/ML_TeamProject/images_png"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGI5eHw7xHnS",
        "outputId": "04b46ab2-0350-477a-90f3-359a9ba4bce9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML_TeamProject/images_png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image path and target class index\n",
        "image_path = 'incipient/crop_D25_2f21093e-60a5-11ec-8402-0a7404972c70.png'\n",
        "class_idx = 0\n",
        "\n",
        "# Get Grad-CAM image\n",
        "grad_cam_image = vit_grad_cam(image_path, class_idx)\n",
        "print(grad_cam_image.shape)\n",
        "\n",
        "# Display Grad-CAM image\n",
        "# img = Image.fromarray(grad_cam_image)\n",
        "# display(img)\n",
        "def display_heatmaps(classified_images,titles):\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(15):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.5 , wspace = 0.3)\n",
        "        plt.imshow(classified_images[i])\n",
        "        plt.title(titles[i])\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "tZXXt9C3gl-R",
        "outputId": "0708e73c-e95b-4f2b-b826-e1e46cff3ddd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'cpu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-9cf0784a2eca>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get Grad-CAM image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgrad_cam_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvit_grad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_cam_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-4a5f5da28d3d>\u001b[0m in \u001b[0;36mvit_grad_cam\u001b[0;34m(image_path, class_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtarget_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use the last encoder layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Visualize Grad-CAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ML_TeamProject/images_png/incipient/pytorch-grad-cam/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 input_tensor, targets, eigen_smooth)\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return self.forward(input_tensor,\n\u001b[0m\u001b[1;32m    193\u001b[0m                             targets, eigen_smooth)\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ML_TeamProject/images_png/incipient/pytorch-grad-cam/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     81\u001b[0m                                                    requires_grad=True)\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ML_TeamProject/images_png/incipient/pytorch-grad-cam/pytorch_grad_cam/activations_and_gradients.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    572\u001b[0m         )\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 )\n\u001b[1;32m    403\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/ML_TeamProject/images_png/incipient/pytorch-grad-cam/pytorch_grad_cam/activations_and_gradients.py\u001b[0m in \u001b[0;36msave_activation\u001b[0;34m(self, module, input, output)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'cpu'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_heatmaps(grad_cam_image,\"titles\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "Tk1KB1K7xRXW",
        "outputId": "da7a19a2-89c1-4582-eb6b-4786d6134bfd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-972575d1b55e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_heatmaps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_cam_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"titles\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-20e4e531f93e>\u001b[0m in \u001b[0;36mdisplay_heatmaps\u001b[0;34m(classified_images, titles)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mwspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassified_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LustxB1qJUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}